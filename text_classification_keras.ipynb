{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoninCamus/AI_ChatBot/blob/master/text_classification_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW1jDJk7aK83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my first notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbuxPIF7a-VO",
        "colab_type": "code",
        "outputId": "539aa1b0-7d6e-4c3a-fd23-1d481d35b529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#mout my drive, where i put all the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEsn7n7Sa7iG",
        "colab_type": "code",
        "outputId": "fdd86f2f-e983-4612-d0ef-00c798dc03f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "#includes our set and 2 others (imdb, yealp)\n",
        "#reach my drive and check if it is there: \n",
        "!ls \"/content/gdrive/My Drive/Colab Notebooks/dataset/sentiment labelled sentences/sentiment labelled sentences\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amazon_cells_labelled.txt  imdb_labelled.txt  readme.txt  yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N41i5XrTbOpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I will now try to follow this tutorial:\n",
        "#https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3022725b-984e-464a-c327-46df70cee101",
        "id": "9BO-mdqOeKI0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer                    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "#read the data in a pandas dataframe\n",
        "path = \"/content/gdrive/My Drive/Colab Notebooks/dataset/sentiment labelled sentences/sentiment labelled sentences/\"\n",
        "filepath_dict = {'yelp': 'yelp_labelled.txt' ,'amazon': 'amazon_cells_labelled.txt','imdb': 'imdb_labelled.txt'}\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "  filepath = path + filepath\n",
        "  df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "  #Add another column filled with the source name\n",
        "  df['source'] = source \n",
        "  df_list.append(df)\n",
        "df = pd.concat(df_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YndBefbSgyRQ",
        "colab_type": "code",
        "outputId": "25a5c80d-f123-4b1b-f2eb-9201713cfdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            sentence  label source\n",
            "0                           Wow... Loved this place.      1   yelp\n",
            "1                                 Crust is not good.      0   yelp\n",
            "2          Not tasty and the texture was just nasty.      0   yelp\n",
            "3  Stopped by during the late May bank holiday of...      1   yelp\n",
            "4  The selection on the menu was great and so wer...      1   yelp\n",
            "                                              sentence  label source\n",
            "743  I just got bored watching Jessice Lange take h...      0   imdb\n",
            "744  Unfortunately, any virtue in this film's produ...      0   imdb\n",
            "745                   In a word, it is embarrassing.        0   imdb\n",
            "746                               Exceptionally bad!        0   imdb\n",
            "747  All in all its an insult to one's intelligence...      0   imdb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFYwSK35jehl",
        "colab_type": "code",
        "outputId": "95ba8926-7346-425c-a4de-849f5ee30057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "df = df #use the whole data\n",
        "# df = df[df['source'] == 'amazon'] #use only amazon\n",
        "\n",
        "sentences = df['sentence'].values #extract sentences\n",
        "y = df['label'].values #extract labels\n",
        "\n",
        "#split datasets intto training and testing\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#turn words into sequence of ints\n",
        "#only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=5000) \n",
        "\n",
        "#learn word_counts, word_docs, word_index (which we will use), document_count:\n",
        "tokenizer.fit_on_texts(sentences_train) \n",
        "\n",
        "print('before vectorization:', sentences_train[0])\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "print('after vectorization:', X_train[0])\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1                          \n",
        "\n",
        "maxlen = 100\n",
        "#pad them into the same lengths\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print('after padding:', X_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before vectorization: I'd rather eat airline food, seriously.\n",
            "after vectorization: [278, 295, 212, 1907, 39, 349]\n",
            "after padding: [ 278  295  212 1907   39  349    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw0fXTPakxIE",
        "colab_type": "code",
        "outputId": "e96ad366-d44d-4d16-ceaf-d5d0519b2ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "embedding_dim = 100 #from tutorial, why?\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Zj1ZADgniG",
        "colab_type": "text"
      },
      "source": [
        "## Comments on the loss function: \n",
        "We use binary_crossentropy because our classifier is binary.\n",
        "([explained here](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a))\n",
        "\n",
        "\\begin{equation}\n",
        "L(y,\\hat{y}) = -\\frac{1}{N}\\sum_{i=0}^{N}(y\\cdot \\log(\\hat{y}_i)+(1-y)\\cdot\\log(1-\\hat{y}_i)\n",
        "\\end{equation}\n",
        "\n",
        "![image:](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kogk-ZhVd2I7",
        "colab_type": "code",
        "outputId": "fc89a9e5-e88d-4bc2-9670-356fd5ef49af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        }
      },
      "source": [
        "#train\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2061 samples, validate on 687 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2061/2061 [==============================] - 5s 3ms/step - loss: 0.6452 - acc: 0.6157 - val_loss: 0.4662 - val_acc: 0.8035\n",
            "Epoch 2/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.3216 - acc: 0.8826 - val_loss: 0.3747 - val_acc: 0.8457\n",
            "Epoch 3/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.1012 - acc: 0.9709 - val_loss: 0.4086 - val_acc: 0.8472\n",
            "Epoch 4/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.0289 - acc: 0.9937 - val_loss: 0.4751 - val_acc: 0.8486\n",
            "Epoch 5/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.0112 - acc: 0.9981 - val_loss: 0.5240 - val_acc: 0.8515\n",
            "Epoch 6/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.0046 - acc: 0.9995 - val_loss: 0.5714 - val_acc: 0.8443\n",
            "Epoch 7/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5898 - val_acc: 0.8472\n",
            "Epoch 8/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 7.3135e-04 - acc: 1.0000 - val_loss: 0.6171 - val_acc: 0.8457\n",
            "Epoch 9/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 4.9520e-04 - acc: 1.0000 - val_loss: 0.6409 - val_acc: 0.8501\n",
            "Epoch 10/10\n",
            "2061/2061 [==============================] - 4s 2ms/step - loss: 3.6301e-04 - acc: 1.0000 - val_loss: 0.6623 - val_acc: 0.8501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clzl39aigGwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoninCamus/AI_ChatBot/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e8HrL9MsH0Z",
        "colab_type": "text"
      },
      "source": [
        "# Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQWu0eWCr78_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7d54bf99-66e7-45ed-a925-58760760d5b3"
      },
      "source": [
        "# Imports \n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voHdeRtZsBK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_pandas(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['Sentence'] = data['Sentence'].str.lower()\n",
        "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
        "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
        "    for index, row in data.iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = df_.append({\n",
        "            \"index\": row['index'],\n",
        "            \"Class\": row['Class'],\n",
        "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
        "        }, ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS8J3FG7uyc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data\n",
        "url = \"https://raw.githubusercontent.com/AntoninCamus/AI_ChatBot/master/provided/amazon_data.csv\"\n",
        "data = pd.read_csv(url, delimiter='\\t', header=None)\n",
        "data.columns = ['Sentence', 'Class']\n",
        "data['index'] = data.index # add new column index\n",
        "columns = ['index', 'Class', 'Sentence']\n",
        "\n",
        "# Transform it into training and validation sets and labels\n",
        "data = preprocess_pandas(data, columns)\n",
        "training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "        data['Sentence'].values.astype('U'),\n",
        "        data['Class'].values.astype('int32'),\n",
        "        test_size=0.10,\n",
        "        random_state=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "# Vectorize data using TFIDF and transform for PyTorch for scalability\n",
        "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
        "training_data = word_vectorizer.fit_transform(training_data)  # transform texts to sparse matrix\n",
        "training_data = training_data.todense() # convert to dense matrix for Pytorch\n",
        "vocab_size = len(word_vectorizer.vocabulary_)\n",
        "validation_data = word_vectorizer.transform(validation_data)\n",
        "validation_data = validation_data.todense()\n",
        "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
        "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
        "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
        "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsnaP8XAvSey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1c223874-84db-44bf-ed92-dd8a3aa37123"
      },
      "source": [
        "training_data"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpf2YVZvxWvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}